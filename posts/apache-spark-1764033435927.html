<h1>Apache Spark</h1><p><i>11/24/2025, 8:17:15 PM</i></p><hr><div><b>Chapter 3. A Tour of Spark’s Toolset</b></div><ul><li><b>withColumnRenamed</b> method that takes two
arguments, the original column name and the new column name. Of course, this doesn’t perform
computation: this is just another transformation!</li><li>&nbsp;DataFrame methods will
accept strings (as column names) or Column types or expressions. Columns and expressions are
actually the exact same thing.</li><li>Spark also makes it easy to turn your
interactive exploration into production applications with spark-submit, a built-in command-line
tool. </li><li>spark-submit does one thing: it lets you send your application code to a cluster and launch it to
execute there. Upon submission, the application will run until it exits (completes the task) or
encounters an error. You can do this with all of Spark’s support cluster managers including
Standalone, Mesos, and YARN.</li><li>spark-submit offers several controls with which you can specify the resources your application
needs as well as how it should be run and its command-line arguments.</li></ul>./bin/spark-submit \&nbsp;<div>--master local \&nbsp;</div><div>./examples/src/main/python/pi.py 10</div><div><ul><li>By changing the master argument of spark-submit, we can also submit the same application to a
cluster running Spark’s standalone cluster manager, Mesos or YARN.</li></ul><div><b>Datasets: Type-Safe Structured APIs</b></div></div><div>a type-safe version of Spark’s structured API called <b>Datasets</b>, for
writing <b>statically typed code in Java and Scala.</b> The Dataset API is <b>not available in Python and R</b>,
because those languages are <b>dynamically typed.</b></div><div><b><br></b></div><div><ul><li><b>Dataframe&nbsp;</b>are a distributed collection of objects
of type Row that can hold various types of tabular data.</li><li>The Dataset API gives users the ability to
assign a Java/Scala class to the records within a DataFrame and manipulate it as a collection of typed
objects, similar to a Java ArrayList or Scala Seq.</li><li>The APIs available on Datasets are type-safe,
meaning that you cannot accidentally view the objects in a Dataset as being of another class than the
class you put in initially. This makes Datasets especially attractive for writing large applications,
with which multiple software engineers must interact through well-defined interfaces.</li><li>The Dataset class is parameterized with the type of object contained inside: Dataset<t> in Java
and <b>Dataset[T] </b>in Scala. For example, a Dataset[Person] will be guaranteed to contain objects of
class Person.&nbsp;</t></li><li><t>The supported types are classes following the <b>JavaBean pattern</b> in
Java and <b>case classes</b> in Scala. These types are restricted because Spark needs to be able to
automatically analyze the type <b>T</b> and <b>create an appropriate schema for the tabular data within your
Dataset.</b></t></li></ul><div>// in Scala&nbsp;</div><div>case class Flight(</div><div>DEST_COUNTRY_NAME: String,&nbsp;</div><div>ORIGIN_COUNTRY_NAME: String,&nbsp;</div><div>count: BigInt</div><div>)&nbsp;</div><div>val flightsDF = spark.read
.parquet("/data/flight-data/parquet/2010-summary.parquet/")&nbsp;</div><div>val flights = flightsDF.as[Flight]&nbsp;<b></b></div></div><div><br></div><div><b>Structured Streaming&nbsp;</b></div><div>Structured Streaming is a high-level API for stream processing that became production-ready in
Spark 2.2. With Structured Streaming, you can take the same operations that you perform in batch
mode using Spark’s structured APIs and run them in a streaming fashion. This can reduce latency and
allow for incremental processing.&nbsp;</div><div><br></div><div>df_var = spark.read.format("csv").option("inferSchema","true").option("header","true").load("path/to/the/file")</div><div>df_var.createOrReplaceTempView("table_name")</div><div>df_schema= df_var.schema</div><div><br></div><div><b>df_var</b>.selectExpr("Customer_id", "unit_price* quantity as revenue","Invoice_date").\</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;groupBy(col("Customer_id", window(col("Invoice_date"),"1 day")). sum(revenue).take(5)</div><div><br></div><div><b>streamingDataFrame </b>= spark.readStream\
.schema(staticSchema)\
.option("maxFilesPerTrigger", 1)\
.format("csv")\
.option("header", "true")\
.load("/data/retail-data/by-day/*.csv")</div><div><br></div><div>streamindDataFrame.isStreaming ----should result in <b>True</b></div><div><b><br></b></div><div>we can always write this data into any other source or tables.</div><div><br></div><div><b>purchaseByCustomerPerHour </b>= streamingDataFrame\
.selectExpr(
"CustomerId",
"(UnitPrice * Quantity) as total_cost",
"InvoiceDate")\
.groupBy(
col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
.sum("total_cost")</div><div><br></div><div><ul><li>Streaming actions are a bit different from our conventional static action because we’re going to be
populating data somewhere instead of just calling something like count (which doesn’t make any
sense on a stream anyways).&nbsp;</li><li><br></li><li>The action we will use will output to an in-memory table that we will
update after each trigger.</li><li><br></li><li>Spark will mutate the data in the in-memory table such that we will always have the highest
value as specified in our previous aggregation</li></ul><div>purchaseByCustomerPerHour.writeStream\
.format("memory")\&nbsp;</div><div>.queryName("customer_purchases")\ .trigger(availableNow=true)</div><div>.outputMode("complete")\
.start()</div></div><div><br></div><div><b>Machine Learning and Advanced Analytics: MLlib</b></div><div><b><br></b></div><div>Spark provides a sophisticated machine learning
API for performing a variety of machine learning tasks, from classification to regression, and
clustering to deep learning.</div><div><br></div><div>WHAT IS K-MEANS?
-means is a clustering algorithm in which “” centers are randomly assigned within the data. The points closest to that
point are then “assigned” to a class and the center of the assigned points is computed. This center point is called the
centroid. We then label the points closest to that centroid, to the centroid’s class, and shift the centroid to the new center of
that cluster of points. We repeat this process for a finite set of iterations or until convergence (our center points stop
changing).&nbsp;<b></b></div><div><br></div><div>staticDataFrame.printSchema() -- this gives the schema read from the data.</div><div><br></div><div><b>Lower-Level APIs</b></div><div><b><br></b></div><div>Spark includes a number of lower-level primitives t<b>o allow for arbitrary Java and Python object
manipulation</b> via Resilient Distributed Datasets (RDDs).&nbsp;</div><div>Virtually everything in Spark is built on top
of <b>RDDs</b>.<b></b></div><div><br></div><div>DataFrame operations are built on top of RDDs and
compile down to these lower-level tools for convenient and extremely efficient distributed execution.</div><div><br></div><div>Use RDDs for, especially when you’re reading or manipulating
raw data, but for the most part you should stick to the <b>Structured APIs</b>.&nbsp;</div><div><br></div><div>RDDs are lower level than
DataFrames because they reveal physical execution characteristics (like partitions) to end users.</div><div><br></div><div>One thing that you might use RDDs for is to parallelize raw data that you have stored in memory on
the driver machine.&nbsp;</div>