<h1>AWS</h1><p><i>11/25/2025, 9:10:23 PM</i></p><hr><div><b>Storage:</b></div><div><br></div><div>Amazon has a cloud storage service called as S3 or buckets.</div><div><br></div><div>S3 Stands for <b>Simple Storage Service</b></div><div><br></div><div>Cost effective and simple storage</div><div><br></div><div>Buckets( containers of storage) and files(object storage)</div><div><br></div><div>Each bucket is created in a region, if latency is a key parameter then the bucket is stored in a datacenter region that is close to where the application is accessed, they can also be created in other regions if there are any compliance regulations.</div><div><br></div><div>Each bucket has to have a <b>global unique name</b> with a specified rules of length and pattern of the name.</div><div><br></div><div>Each object has a unique value called <b>Key </b>which is used to identify that object</div><div><br></div><div>If I upload a file directly into the bucket then filename.csv is the <b>key</b>, if upload into folder/filename.csv then <b>folder/filename.csv </b>will be my key to identify and observation is that key has not included the container/bucket name in it.</div><div><br></div><div>Based on the use case we can divide the storage classes, which differ from each other by Durability, Availability, Lifecycle Rules, Versioning.</div><div><br></div><div>S3 Standard, S3 Glacier(For archival data storage) etc</div><div><br></div><div><b>Data Ingestion Methods:</b></div><div><br></div><div><b>Batch V/S Streaming</b></div><div>Glue ------- Amazon Kenesis</div><div>cost effective &amp; less hustle ---- costly</div><div>ingests periodically------ time sensitive ingestion</div><div><br></div><div><b>AWS Glue:</b></div><div><b>Fully managed ETL tool</b> designed to make it easy to load and transform data</div><div><br></div><div>Easily create jobs using visual interface and various integration services to redshift, athena etc</div><div><br></div><div><b>Automatic script is generated</b> when we try to run on the <b>apache spark cluster</b>.( <b>serverless</b>, feature managed everything, pay as you go)</div><div><br></div><div>Another major component of Glue is<b> Amazon Glue Data catalog</b>, which can extract the information of the <b>schema,metadata </b>of the file in the S3 bucket and store it in the centralized <b>metastore </b>in AWS, by doing this the Analytical tools like redshift, athena, ems can use this to query on the S3 bucket in the ETL jobs.</div><div><br></div><div>This can be done in <b>ETL jobs or also using Amazon Crawler</b></div><div><br></div><div><b>Glue Crawlers:</b></div><div><ol><li>Scan the data source</li><li>infer the schema</li><li>store the schema in the catalog</li></ol></div><div><br></div><div>ETL jobs and Crawlers can be run by <b>scheduling or trigger based or incremental upload</b>(only new data is uploaded) based on the last crawling time.</div><div><br></div><div><b>Task:&nbsp;</b></div><div><ol><li><b>Created an S3 bucket, with a folder, and uploaded csv file into it.</b></li><li><b>Open Glue service, go to catalog and look for crawlers, create crawler by adding the source folder( if all the files inside are having same structure).</b></li><li><b>Create a role in the configurations by adding the database and table prefix where the crawled data is stored.</b></li><li><b>After creating Run the crawler to create the table and query it using Athena.</b></li></ol><div><b>AWS ATHENA:</b></div></div><div>An interactive tool to query files in S3 using SQL, it is a serverless architecture.</div><div><br></div><div>Use Cases:</div><div><ul><li>Log Analysis</li><li>Ad-hoc Analysis</li><li>Data Lake Analytics</li><li>Real time analytics</li></ul><div><b>Task:</b></div></div><div><ol><li><b>Go to the table in the catalog and look for preview the data in actions which takes to athena query editor.</b></li><li><b>create a folder or storage location for the query results to be stored</b></li><li><b>run the query to find the average age&nbsp;</b></li></ol><div><b>Federated Query:</b></div><div><b><br></b></div><div>Apart from using the data in S3 to query Athena can also use other sources using a connector</div></div><div><ul><li>Relational, non relational source</li><li>custom source</li><li>object data source</li></ul><div>Performance and Cost:</div></div><div><br></div><div>Partition can be done to prune the data based on the column</div><div>Athena also supports partition projection where automatically it can choose the partition column based on the query history</div><div>AWS Glue partition Indexes helps to get only subset of partition columns instead of loading all the data from the catalog.</div><div><br></div><div>Query reuse option in the query editor</div><div><br></div><div>Data compression: reducing the file size to speed up queries</div><div><br></div><div>Format compression: changing the data into an optimized structure such as apache parquet, ORC like columnar formats.</div><div><br></div><div><b>Workgroups:</b></div><div><br></div><div>Isolate queries from other queries in the same account</div><div><br></div><div>Isolate queries for different Teams, Users, Applications</div><div><br></div><div><b>Control</b></div><div>query execution settings &amp; access &amp; cost &amp; type of engine( Athena SQL VS Apache Spark)</div><div><br></div><div>by default 1000 workgroups can be created per region</div><div><br></div><div>Each account has a primary workgroup which cannot be deleted. By default it has access to all. we can also create a new workgroup and assign IAM roles and policies accordingly.</div>